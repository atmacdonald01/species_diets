{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define diet categories\n",
    "carnivore = [\"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod'] # 'predator']  #how to deal with words like \"predator\" that are used in contradictory contexts?\n",
    "herbivore = [\"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'tortoise', 'iguana']\n",
    "omnivore = [\"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\", 'omnivores']\n",
    "insectivore = [\"insectivore\", \"Insectivore\", 'insectivorous', \"Insectivorous\", 'entomophagous', 'spider']\n",
    "frugivore = [\"frugivore\", \"Frugivore\", 'frugivorous', 'Frugivorous']\n",
    "bloodfeeder = [\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\"]\n",
    "scavenger = [\"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous']\n",
    "granivore = [\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\"]\n",
    "nectarivore = [\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\"]\n",
    "folivore = [\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\"]\n",
    "gummivore = [\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\"]\n",
    "filterfeeder = [\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ']\n",
    "#and a category for food-related words that aren't specific to one diet\n",
    "undefined = [' eat', ' food ', ' consume', ' feed', ' diet ', ' prey']\n",
    "\n",
    "#make a master list of all query strings\n",
    "all_diet_query=carnivore + herbivore + omnivore + insectivore + frugivore + bloodfeeder + scavenger + granivore + nectarivore + folivore + gummivore + filterfeeder + undefined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories for post-processing\n",
    "\n",
    "#definining new sets of keywords\n",
    "adhoc_carnivore = [\"fish\", \"meat\", \"invertebrates\", \"insects\", \"crabs\", \"crustaceans\", \"snake\", \"mammal\", \"frog\", \"bird\", \"arthropod\", \"caterpillar\", \"beetle\", \"prey\"]\n",
    "adhoc_herbivore = [\"fruit\", \"seed\", \"flower\", \"leaves\", \"leaf,\", \"plant\", \"wood\", \"nectar\", \"tree\", \"shrub\", \"twig\", \"foliage\", \"bark\", \"berries\", \"leaf litter\", \"pollen\", \"seeds\", \"algae\", \"mosses\", \"sap\", \"brassicas\", \"spurge\", \"lichens\", \"iguana\", \"tortoise\"]\n",
    "adhoc_omnivore = [\"eggs\", \"fungi\"]\n",
    "adhoc_scavenger = [\"dung\", \"feces\", \"faeces\"]\n",
    "adhoc_diet_query = (adhoc_carnivore, adhoc_herbivore, adhoc_omnivore, adhoc_scavenger)\n",
    "keyphrases = [\"feed on\", \"feeds on\", \"feeding on\"]\n",
    "\n",
    "#create the inverse of the ad hoc lists so that later we can reverse-categorize species in m_diet_words based on what specific foods \n",
    "#they were found to consume\n",
    "searchable_diet_keywords = {}\n",
    "searchable_diet_keywords['carnivore'] = [\"fish\", \"meat\", \"invertebrates\", \"insects\", \"crabs\", \"crustaceans\", \"snake\", \"mammal\", \"frog\", \"bird\", \"arthropod\", \"caterpillar\", \"beetle\", \"prey\", \"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod']\n",
    "searchable_diet_keywords['herbivore'] = [\"fruit\", \"seed\", \"flower\", \"leaves\", \"leaf,\", \"plant\", \"wood\", \"nectar\", \"tree\", \"shrub\", \"twig\", \"foliage\", \"bark\", \"berries\", \"leaf litter\", \"pollen\", \"seeds\", \"algae\", \"mosses\", \"sap\", \"brassicas\", \"spurge\", \"lichens\", \"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'tortoise', 'iguana']\n",
    "searchable_diet_keywords['omnivore'] = [\"eggs\", \"fungi\", \"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\"]\n",
    "searchable_diet_keywords['scavenger'] = [\"dung\", \"feces\", \"faeces\", \"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous']\n",
    "searchable_diet_keywords['insectivore'] = [\"insectivore\", \"Insectivore\", 'insectivorous', \"Insectivorous\", 'entomophagous', 'spider']\n",
    "searchable_diet_keywords['frugivore'] = [\"frugivore\", \"Frugivore\", 'frugivorous', 'Frugivorous']\n",
    "searchable_diet_keywords['bloodfeeder'] = [\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\"]\n",
    "searchable_diet_keywords['granivore'] = [\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\"]\n",
    "searchable_diet_keywords['nectarivore'] = [\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\"]\n",
    "searchable_diet_keywords['folivore'] = [\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\"]\n",
    "searchable_diet_keywords['gummivore'] = [\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\"]\n",
    "searchable_diet_keywords['filterfeeder'] = [\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ']\n",
    "\n",
    "#define broad categories--so e.g. something that's both a frugivore and a folivore is an herbivore\n",
    "broad_herbivore = ['frugivore', 'herbivore', 'granivore', 'nectarivore', 'folivore']\n",
    "broad_omnivore = ['omnivore', 'gummivore', 'filter feeder']\n",
    "broad_carnivore = ['carnivore', 'insectivore', 'bloodfeeder', 'scavenger']\n",
    "\n",
    "#for ease of use later, combine these broad categories into a dictionary\n",
    "broad_categories = {}\n",
    "broad_categories['herbivore'] = broad_herbivore\n",
    "broad_categories['omnivore'] = broad_omnivore\n",
    "broad_categories['carnivore'] = broad_carnivore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that takes a .csv file with species data and returns that data as a dictionary\n",
    "#values can be adjusted depending on how csv is organized\n",
    "def species_dict(file):\n",
    "    \n",
    "    #a list to hold the species in the file\n",
    "    species_list = []\n",
    "    \n",
    "    #open the file and read the values into our \n",
    "    with open(file, \"r\") as metadata:\n",
    "        \n",
    "        #skip the first line, since that's just the title line\n",
    "        next(metadata)\n",
    "        \n",
    "        csvreader = csv.reader(metadata)\n",
    "        \n",
    "        #for each row, make a dictionary for the species\n",
    "        for row in csvreader:\n",
    "            species = {}\n",
    "            species['species_name'] = row[0].strip()    #values can be adjusted depending on layout of file being read\n",
    "            species['common_name'] = row[1].strip()\n",
    "            species['class_name'] = row[2].strip()\n",
    "            \n",
    "            #and add it to the species_list, avoiding repeats\n",
    "            if species not in species_list:\n",
    "                species_list.append(species)\n",
    "            \n",
    "    return species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using html/BeautifulSoup, scrape the Encyclopedia of Life website for diet data\n",
    "#this function looks up the species in EOL, checks the \"eat\" section of its page, looks at the species overview, and looks at\n",
    "#its \"data\" section for information about its data\n",
    "\n",
    "def check_eol(species):\n",
    "    diet = []                        #establish diet list\n",
    "    og_species = species.lower()     #and species name\n",
    "    \n",
    "    no_eats = False\n",
    "    \n",
    "    #first, build the search URL from the species name\n",
    "    species = species.lower().split(\" \")\n",
    "    species = \"+\".join(species)\n",
    "    url = \"https://eol.org/search?utf8=%E2%9C%93&q=\" + species\n",
    "    \n",
    "    #go to the url\n",
    "    page = requests.get(url)      #use 'requests' library to grab the html code of the url\n",
    "    #and get the html\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")      #get the html content of the page using BeautifulSoup\n",
    "    \n",
    "    #get the list of search results\n",
    "    search = soup.find(class_=\"search-results js-search-results\")                    #pick out the results section from the html\n",
    "    if search != None:\n",
    "        results_list = search.find_all(\"a\")                                             #pick out the list of search results\n",
    "    \n",
    "    \n",
    "        #find the result with the most associated media\n",
    "        highest = 0\n",
    "        choice = \"\"\n",
    "        species_link = \"\"\n",
    "        for item in results_list:                                        #for each result, find the species name and how many articles are associated with it\n",
    "            link = item['href']                                        #get the link for the result\n",
    "            result = item.find(\"div\", class_=\"search-title\").text       #get the search title (the text of the result)\n",
    "            if og_species in result.lower():                           #if the result has the correct species name, look at it\n",
    "                media = item.find(\"ul\", class_=\"resource-bubbles\")\n",
    "                total_media = media.text.split(\"MEDIA\")[0].strip()            #split off the first number (the # of associated media) from other numbers\n",
    "                if total_media.isdigit() and int(total_media) > highest:                                   #if this number is a true number (not \\n or '1 MEDIUM', for ex)\n",
    "                    highest = int(total_media)\n",
    "                    choice = item\n",
    "                    species_link = \"http://eol.org\" + link #+ \"/data\"         #get the address of the page with the highest results and go to the \"data\" section\n",
    "            \n",
    "        #now that we have a species page, we need to check the overview, the \"data\" page, and the \"articles\" page for diet information\n",
    "        if len(species_link) > 0:\n",
    "            \n",
    "            #####\n",
    "            #the first step is to go to the overview page and see if there's a diet category there\n",
    "            general_page = requests.get(species_link)\n",
    "            general_soup = BeautifulSoup(general_page.content, \"html.parser\")\n",
    "            general = general_soup.find(\"p\")\n",
    "            try:\n",
    "                general_info = general.text.lower()\n",
    "                for item in all_diet_query:\n",
    "                    #if we find a diet type, get the sentence and add it to \"diet\"\n",
    "                    if item in general_info:\n",
    "                        sentence = \"\"\n",
    "                        index = general_info.index(item)\n",
    "                        stop = False\n",
    "                        #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                        while stop == False:\n",
    "                            try:\n",
    "                                sentence += general_info[index]\n",
    "                            except IndexError:\n",
    "                                break\n",
    "                            index += 1\n",
    "                            try:\n",
    "                                if general_info[index] == \".\":\n",
    "                                    stop = True\n",
    "                            except IndexError:\n",
    "                                break\n",
    "                        diet.append(sentence)             #add the sentence to the diet list\n",
    "            except AttributeError:\n",
    "                print(\"No overview found.\")\n",
    "\n",
    "            ######\n",
    "            #the next step is to go to the \"data\" page and see if there's an \"eat\" section, so we'll get the html content of the species data page\n",
    "            species_data_link = species_link + \"/data\"\n",
    "            data_page = requests.get(species_data_link)\n",
    "            data_soup = BeautifulSoup(data_page.content, \"html.parser\")\n",
    "        \n",
    "            #find the \"eat\" category out of the \"data\" section on the species page\n",
    "            categories = data_soup.find_all(\"a\", class_=\"item\")             #find all the data categories on the \"data\" page of the species\n",
    "            diet_link = \"\"\n",
    "            for category in categories:\n",
    "                if category.text == \"eat\":\n",
    "                    diet_link = \"http://eol.org\" + category['href']             #if there's an \"eat\" section, get the diet page url\n",
    "    \n",
    "            #if there was an \"eat\" section, get the html content of the diet page\n",
    "            if len(diet_link) > 0:\n",
    "                diet_page = requests.get(diet_link)\n",
    "                diet_soup = BeautifulSoup(diet_page.content, \"html.parser\")\n",
    "    \n",
    "                #add each eaten species to the diet list\n",
    "                eats = diet_soup.find_all(\"div\", class_=\"trait-val\")       #find the listed species that species eats\n",
    "                for eat in eats:\n",
    "                    diet.append(eat.text.strip())\n",
    "        \n",
    "            #if there's no \"eat\" section, say so\n",
    "            else:\n",
    "                print(\"No eats found.\")\n",
    "                no_eats = True\n",
    "            \n",
    "            ########\n",
    "            #if diet is still empty, the next step is to check the \"articles\" section for diet information\n",
    "            if no_eats == True and diet == []:\n",
    "                species_article_link = species_link + \"/articles\"\n",
    "                article_page = requests.get(species_article_link)\n",
    "                article_soup = BeautifulSoup(article_page.content, \"html.parser\")\n",
    "\n",
    "                #now that we have the html of the \"articles\" page, we need to get its text and look through it for diet keywords\n",
    "                article_text = []                                     # a list to hold the text of the articles\n",
    "                articles = article_soup.find_all(\"div\", class_=\"body uk-margin-small-top\")        #get the article sections\n",
    "                #and add each article to the list of article_text\n",
    "                for article in articles:\n",
    "                    article_text.append(article.text)\n",
    "\n",
    "                #then, go through each item in article_text and look for diet information\n",
    "                for a in article_text:\n",
    "                    for item in all_diet_query:                #loop through our list of diet types and see if they're mentioned in the page\n",
    "                        #if we find a diet type in the article\n",
    "                        if item in a:\n",
    "                            sentence = \"\"\n",
    "                            index = a.index(item)\n",
    "                            stop = False\n",
    "                            #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                            while stop == False:\n",
    "                                try:\n",
    "                                    sentence += a[index]\n",
    "                                except IndexError:\n",
    "                                    break\n",
    "                                index += 1\n",
    "                                try:\n",
    "                                    if a[index] == \".\":\n",
    "                                        stop = True\n",
    "                                except IndexError:\n",
    "                                    break\n",
    "                            diet.append(sentence)             #add the sentence to the diet list\n",
    "\n",
    "            ######\n",
    "            #if diet is still empty, try looking up the classification tree\n",
    "            tree = general_soup.find(\"div\", class_=\"hier js-hier-summary\")\n",
    "            try:\n",
    "                branches = tree.find_all(\"a\")\n",
    "                #look at each step in the tree and add it to diet if it matches a diet type (amphibians, carnivores, etc.)\n",
    "                for branch in branches:\n",
    "                    if branch.text.lower() in all_diet_query:\n",
    "                        diet.append(branch.text.lower())\n",
    "            except AttributeError:\n",
    "                print(\"No tree found.\")\n",
    "            \n",
    "            \n",
    "            #######                \n",
    "            #if we go through all that and still have no diet info, say so\n",
    "            if no_eats == True and diet == []:\n",
    "                print(\"No diet section and no article diet data.\")\n",
    "                diet = \"No diet section and no article diet data.\"\n",
    "        \n",
    "        #if there's no species link, say so\n",
    "        else:\n",
    "            print(\"No species link found.\") \n",
    "            diet = \"No species link found.\"\n",
    "        \n",
    "    #if there's no results page, say so\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "        diet = \"No results found.\"\n",
    "        \n",
    "    return diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to search wikipedia: takes a species name and returns a dictionary of its diet data\n",
    "def check_wiki(species):\n",
    "    species = species.strip()   #clean species name of any extraneous spaces\n",
    "    \n",
    "    #establish dictionary and count of how many diet-type instances found\n",
    "    diet = {}\n",
    "    total_count = 0\n",
    "    \n",
    "    #look for the species in wikipedia\n",
    "    try:\n",
    "        page = wikipedia.page(species, auto_suggest=False)                 #get page, if possible\n",
    "        page_content = page.content                                        #pull the content of the page\n",
    "        for item in all_diet_query:                #loop through our list of diet types and see if they're mentioned in the page\n",
    "            #if we find a diet type in page_content\n",
    "            if item in page_content:\n",
    "                #make an entry for each diet type that is found\n",
    "                if item not in diet.keys():        \n",
    "                    diet[item] = {}    \n",
    "                    diet[item][\"count\"] = 0         \n",
    "                    diet[item][\"mentions\"] = []\n",
    "                    \n",
    "                #add diet type to \"diet\" dictionary with spots for counts of each diet type mentioned and the sentences the mention occurs in\n",
    "                index = page_content.index(item)\n",
    "                sentence = \"\"\n",
    "                stop = False\n",
    "                #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                while stop == False:\n",
    "                    try:\n",
    "                        sentence += page_content[index]\n",
    "                    except IndexError:\n",
    "                        break\n",
    "                    index += 1\n",
    "                    try:\n",
    "                        if page_content[index] == \".\":\n",
    "                            stop = True\n",
    "                    except IndexError:\n",
    "                        break\n",
    "                \n",
    "                #increase the count of the diet that was found and append the sentence to the dictionary\n",
    "                diet[item][\"count\"] += 1\n",
    "                diet[item][\"mentions\"].append(sentence)\n",
    "                \n",
    "                total_count += 1                   #count up that we've found a diet type\n",
    "                \n",
    "                \n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print('Page not found: ' + species)\n",
    "        return None\n",
    "                    \n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        print(\"Ack! Disambiguation error.\")\n",
    "        return None\n",
    "        \n",
    "    #sometimes a KeyError is thrown for some strange reason...\n",
    "    except KeyError:\n",
    "        print(\"KeyError?\")\n",
    "        return None\n",
    "        \n",
    "    #return nothing if no diet types were found; otherwise, return the diet dictionary\n",
    "    if total_count == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return diet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to consolidate the diet types found by the wiki_diets function (e.g. \"predator\", \"carnivorous\" collapsed into \"carnivore\")\n",
    "#also finds the most commonly mentioned diet type for each species\n",
    "\n",
    "def consolidate(diet_dict):\n",
    "    \n",
    "    #first, make a dictionary that consolidates the various vocab (carnivore, carnivorous, etc.) into one diet category\n",
    "    diet_key = {}\n",
    "    diet_key[(\"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod')] = \"carnivore\"\n",
    "    diet_key[(\"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'iguana', 'tortoise')] = \"herbivore\"\n",
    "    diet_key[(\"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\")] = \"omnivore\"\n",
    "    diet_key[(\"insectivore\", \"Insectivore\", \"insectivorous\", \"Insectivorous\", 'entomophagous', 'spider')] = \"insectivore\"\n",
    "    diet_key[(\"frugivore\", \"Frugivore\", \"frugivorous\", \"Frugivorous\")] = \"frugivore\"\n",
    "    diet_key[(\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\")] = \"blood feeder\"\n",
    "    diet_key[(\"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous')] = \"scavenger\"\n",
    "    diet_key[(\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\")] = \"granivore\"\n",
    "    diet_key[(\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\")] = \"nectarivore\"\n",
    "    diet_key[(\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\")] = \"folivore\"\n",
    "    diet_key[(\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\")] = \"gummivore\"\n",
    "    diet_key[(\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ')] = \"filter feeder\"\n",
    "    diet_key[(' eat', ' food ', ' consume', ' diet ', ' prey')] = \"undefined\"\n",
    "    \n",
    "\n",
    "    #then, establish a dictionary that will hold the consolidated diet types for each animal\n",
    "    categorized = {}\n",
    "    \n",
    "    #and go through the diet_dict to consolidate the diets of each species in diet_dict\n",
    "    for species in diet_dict:\n",
    "        categorized[species] = {}         #establish an entry for each species\n",
    "        \n",
    "        #for each diet of each species in diet_dict, look through diet_key and assign a category accordingly\n",
    "        if diet_dict[species]['diet'] != None and diet_dict[species]['diet'] != []:\n",
    "            for diet in diet_dict[species]['diet']:     #for each diet word listed under 'diet' \n",
    "          #      print(diet)\n",
    "                for item in diet_key:                  #go through the diet categories and find which one the diet word fits into\n",
    "                    if diet.lower() in item:\n",
    "                        categorized_entry = diet_key[item]\n",
    "                    \n",
    "                #if need be, establish an entry in 'categorized' for this diet type\n",
    "                if categorized_entry not in categorized[species]:\n",
    "                    categorized[species][categorized_entry] = {}\n",
    "                    categorized[species][categorized_entry]['count'] = 0\n",
    "                    categorized[species][categorized_entry]['sentences'] = []\n",
    "\n",
    "                #add the specific diet count to the category count, and append the relevant sentences\n",
    "                categorized[species][categorized_entry]['count'] += diet_dict[species]['diet'][diet]['count']\n",
    "                categorized[species][categorized_entry]['sentences'].append(diet_dict[species]['diet'][diet]['mentions'])\n",
    "\n",
    "        #after going through all the diets per species, find which diet category has the most mentions\n",
    "        highest = 0\n",
    "        highest_cat = ''\n",
    "        for category in categorized[species]:\n",
    "            if categorized[species][category]['count'] > highest:\n",
    "                highest = categorized[species][category]['count']\n",
    "                highest_cat = category\n",
    "        categorized[species]['most_likely'] = highest_cat\n",
    "        \n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the functions defined above, find the diets of a species list.\n",
    "\n",
    "#read in data\n",
    "species_list = species_dict(\"allison_song_diet.csv\")  #insert name of data file here\n",
    "\n",
    "#avoid repeats\n",
    "species_list_non_dict = []\n",
    "for item in species_list:\n",
    "    name = item['species_name']\n",
    "    if name not in species_list_non_dict:\n",
    "        species_list_non_dict.append(name)\n",
    "        \n",
    "#split the big list into smaller ones so it's easier for the eol function to digest--trying to do thousands of species at once\n",
    "#overloads the function\n",
    "#produces an embedded list: the original list split into chunks 500 species long, and each chunk added to a list\n",
    "small_lists = []\n",
    "working_list = []\n",
    "total_count = 0\n",
    "count = 0\n",
    "for item in species_list_non_dict:\n",
    "    working_list.append(item)\n",
    "    count += 1\n",
    "    total_count += 1\n",
    "    #if the list is 500 long, add that chunk to the master list and start a new chunk\n",
    "    if count == 500:\n",
    "        small_lists.append(working_list)\n",
    "        count = 0\n",
    "        working_list = []\n",
    "    elif total_count == len(species_list_non_dict):\n",
    "        small_lists.append(working_list)\n",
    "        \n",
    "        \n",
    "    \n",
    "#using the check_eol function, create a dictionary of each species and the diet results found from scraping the EOL website\n",
    "eol_species_diet = {}          #dictionary to hold our results\n",
    "\n",
    "#counts to measure results and where data is found or missing\n",
    "count = 0\n",
    "eats_unfound = 0\n",
    "link_unfound = 0\n",
    "species_unfound = 0\n",
    "\n",
    "#go through each mini-list; for each species, grab the scientific name and run the check_eol function on it\n",
    "for list_ in full_test_lists:\n",
    "    count += 1\n",
    "    print(\"NOW CRUNCHING THROUGH LIST: \", count, \"\\n\")\n",
    "    eol_species_diet[count] = {}\n",
    "    for species in list_:\n",
    "        try:\n",
    "            scientific_name = species[0]\n",
    "        except IndexError:\n",
    "            print(\"unhashable species name\")\n",
    "     #   common_name = species[1]\n",
    "        eol_species_diet[count][scientific_name] = []\n",
    "#    eol_species_diet[species] = []\n",
    "        result = check_eol(scientific_name)\n",
    "        \n",
    "        if result == \"No diet section and no article diet data.\":\n",
    "            eats_unfound += 1\n",
    "            \n",
    "        elif result == \"No species link found.\":\n",
    "            link_unfound += 1\n",
    "            \n",
    "        elif result == \"No results found.\":\n",
    "            species_unfound += 1\n",
    "            \n",
    "        else:\n",
    "            eol_species_diet[count][scientific_name] = result\n",
    "            print(species, result)\n",
    "            \n",
    "            \n",
    "print(eats_unfound)\n",
    "print(link_unfound)\n",
    "\n",
    "\n",
    "#next, write the data in eol_species_diet to a .csv; that way it's stored safely and we can read it into other functions\n",
    "\n",
    "with open(\"allison_song_eol_species_data.csv\", \"w\") as f:   #insert name of file here\n",
    "    writer = csv.writer(f)\n",
    "    header = (\"SPECIES\", \"DATA\")    #title of the columns\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    num = 0\n",
    "    row_count = 0\n",
    "    for count in eol_species_diet:                 #for each sub-list of 500 species\n",
    "        for species in eol_species_diet[count]:    #for each species in the sub-list\n",
    "            num += 1\n",
    "            if eol_species_diet[count][species] != []:              #if a diet was found\n",
    "                row = (species, eol_species_diet[count][species])   #make a row of the species and the data that was found\n",
    "                writer.writerow(row)                                #and add it to the .csv\n",
    "                row_count += 1\n",
    "    \n",
    "    #num tells us how many species there were total; row_count shows how many species had diet data\n",
    "    print(num)\n",
    "    print(row_count)\n",
    "\n",
    "\n",
    "#search wikipedia for diets\n",
    "wiki_species_diet = {}         #dictionary to hold species and the diets that are found\n",
    "\n",
    "unfound = 0\n",
    "no_keywords = []\n",
    "\n",
    "#look through each species our species list\n",
    "for species in species_list_non_dict:\n",
    "    \n",
    "    diet = find_diet(species)                 #run find_diet on the species\n",
    "    \n",
    "    #if unsuccessful, try genus name\n",
    "    if diet == None:\n",
    "        genus = species.split()[0]\n",
    "        diet = find_diet(genus)\n",
    "    \n",
    "    #if still unsuccessful, count as unfound\n",
    "    if diet == None:\n",
    "        unfound += 1\n",
    "        no_keywords.append(species)\n",
    "        \n",
    "  #add diet to wiki_diets dictionary\n",
    "    wiki_species_diet[species] = diet\n",
    "        \n",
    "  #  print(diet)\n",
    "    \n",
    "print(unfound)\n",
    "print(no_keywords)\n",
    "print(wiki_species_diet)\n",
    "\n",
    "#consolidate wiki_species_diet using the consolidate function\n",
    "consolidated_wiki_diets = consolidate(wiki_species_diet)\n",
    "\n",
    "#then, write the output of the consolidated dictionary to a .csv, just like we did for the EOL data\n",
    "\n",
    "with open('allison_song_wikipedia_diets.csv', 'w') as f:          #insert name of file here\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"Species name\", \"Most likely diet\", \"All diets mentioned\", \"Diet sentences\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for species in consolidated_wiki_diets:\n",
    "        most_likely = consolidated_wiki_diets[species]['most_likely']\n",
    "        sentences = []\n",
    "        diets = []\n",
    "        #go through each diet listed (carnivore, herbivore, etc.) and add the diet and its related sentences to the diet and sentences lists\n",
    "        for diet in consolidated_wiki_diets[species]:\n",
    "            if diet != 'most_likely' and diet != 'class_name' and diet != 'common_name':\n",
    "       #         count = consolidated_diet[species][diet]['count']\n",
    "                diets.append(diet)\n",
    "                sentences.append(consolidated_diet[species][diet]['sentences'])\n",
    "            \n",
    "        #make a row of the species name, the most likely diet, all the listed diets, and all the diet sentences that were found\n",
    "        row = (species, most_likely, diets, sentences)\n",
    "        \n",
    "        print(row)\n",
    "        writer.writerow(row)\n",
    "\n",
    "        \n",
    "\n",
    "#now that we have both the EOL data and the wikipedia data, we can read both of those files back in and make a consolidated file\n",
    "#with all of the data we have.\n",
    "\n",
    "#####\n",
    "#first, read the EOL data into a dictionary\n",
    "eol_diets = {}\n",
    "with open('allison_song_eol_species_data.csv', 'r') as f:    #open the file we put the eol data in\n",
    "    file = csv.DictReader(f)\n",
    "    for line in file:\n",
    "        name = \"\"\n",
    "        species = line['SPECIES']                     #get the species out of the line\n",
    "  #      print(species)\n",
    "        name = species\n",
    "        eol_diets[name] = {}                         #make an entry in the dictionary for that species\n",
    "        eol_diets[name]['diet'] = line['DATA']       #and fill in the diet based on the eol data we have for that speceis\n",
    "\n",
    "#print to check that it worked\n",
    "#for entry in eol_diets:\n",
    " #   print(entry)\n",
    "  #  print(eol_diets[entry])\n",
    "    \n",
    "    \n",
    "#####\n",
    "#then read the wikipedia data into a dictionary\n",
    "wikipedia_diets = {}\n",
    "with open('allison_song_wikipedia_diets.csv', 'r') as f:\n",
    "    file = csv.DictReader(f)\n",
    "    for line in file:\n",
    "        name = line['Species name']\n",
    "     #   name = (line['Species name'], line['Common name'])\n",
    "        \n",
    "        wikipedia_diets[name] = {}\n",
    "        wikipedia_diets[name]['diet mentions'] = line['Mentions']\n",
    "        wikipedia_diets[name]['likely diet'] = line['Most likely diet']\n",
    "\n",
    "#print to check that it worked\n",
    "#for entry in wikipedia_diets:\n",
    " #   print(entry)\n",
    "  #  print(wikipedia_diets[entry])\n",
    "\n",
    "    \n",
    "#####\n",
    "#then, make a consolidated dictionary of both the EOL and wikipedia data\n",
    "all_diets = {}\n",
    "for species in species_list_non_dict:\n",
    "    #make an entry for each species we have with slots for wikipedia data, eol data, and a diet guess\n",
    "    try:\n",
    "        species = species[0]\n",
    "        all_diets[species] = {}\n",
    "        all_diets[species]['wikipedia'] = []\n",
    "        all_diets[species]['eol'] = []\n",
    "        all_diets[species]['diet_guess'] = \"\"\n",
    "    except IndexError:\n",
    "        print(\"blank\")\n",
    "\n",
    "#go through the wikipedia diets and add the data to all_diets\n",
    "for species in wikipedia_diets:\n",
    "    species = [species]            #make species a list so the indexing works\n",
    "    all_diets[species[0]]['wikipedia'] = wikipedia_diets[species[0]]['diet mentions']\n",
    "    #if there's no diet guess for this species, fill in the wikipedia diet guess\n",
    "    if all_diets[species[0]]['diet_guess'] == \"\":\n",
    "        all_diets[species[0]]['diet_guess'] = wikipedia_diets[species[0]]['likely diet']\n",
    "\n",
    "#go through the eol diets and add the data to all_diets\n",
    "for species in eol_diets:\n",
    "    all_diets[species]['eol'] = eol_diets[species]['diet']\n",
    "\n",
    "    \n",
    "#and print all_diets so we can check it worked\n",
    "for species in all_diets:\n",
    "    print(species)\n",
    "    print(all_diets[species])\n",
    "    \n",
    "    \n",
    "#write the data we have to a .csv\n",
    "with open('allison_song_all_species_data.csv', 'w') as f:               #update file name here\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = (\"species name\", \"Wikipedia data\", \"EOL data\", \"diet guess\")\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for entry in all_diets:\n",
    "        row = \"\"\n",
    "        species = entry\n",
    "        wiki = all_diets[entry]['wikipedia']\n",
    "        eol = all_diets[entry]['eol']\n",
    "        guess = all_diets[entry]['diet_guess']\n",
    "        row = (species, wiki, eol, guess)\n",
    "        writer.writerow(row)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the data is ready for post-processing.\n",
    "\n",
    "#first, we have to read in the species data\n",
    "u_diets = {}         #dictionary to hold all the species with an \"undefined\" diet type\n",
    "\n",
    "with open('allison_song_all_species_data.csv', 'r') as f:     #open the file with the EOL/wikipedia data and the diet guesses for each species\n",
    "    \n",
    "    next(f)    #skip the header\n",
    "    \n",
    "    reader = csv.reader(f)\n",
    "    \n",
    "    for row in reader:\n",
    "        #filter out the undefined diets\n",
    "        if row[-1] == \"undefined\":\n",
    "  #          print(row)\n",
    "            species = row[0]\n",
    "            u_diets[species] = []               #create an entry for the undefined species\n",
    "            if row[1] != []:                    #and add the EOL and wikipedia data, if it's there\n",
    "                u_diets[species].append(row[1])\n",
    "            if row[2] != []:\n",
    "                u_diets[species].append(row[2])\n",
    "\n",
    "                \n",
    "                \n",
    "#first, look through the entries for species that have one of the diet categories listed above. These will be stored in \"diet_words\"\n",
    "#Also record how many times each diet category is mentioned--many of these undefined species fall under multiple categories\n",
    "\n",
    "diet_words = {}\n",
    "\n",
    "#go through the species data and look for existing keywords\n",
    "for species in u_diets:\n",
    "    print(species)\n",
    "    for item in u_diets[species]:    #for each dataset (EOL, wikipedia) for each species\n",
    "        print(item)\n",
    "        for list_ in all_diet_query:   #for the lists of each diet category (carnivore, herbivore, etc.)\n",
    "            for entry in list_:           #for each diet word in the diet category\n",
    "                if entry in item.lower():          #if that diet word is in the EOL or wikipedia dataset\n",
    "                    if species not in diet_words:\n",
    "                        diet_words[species] = {}    #give that species an entry in diet_words\n",
    "                    print(entry, '\\n')\n",
    "                    if entry not in diet_words[species]:  #and add the diet word and its count to the species in diet_words\n",
    "                        diet_words[species][entry] = 0\n",
    "                    diet_words[species][entry] += 1\n",
    "                    \n",
    "                    \n",
    "#the next step is to look at the entries that don't have those convenient keywords and use a more specialized list to try to define\n",
    "#their diet type.\n",
    "\n",
    "m_diets = {}            #m_diets for mystery diets\n",
    "for item in u_diets:\n",
    "    if item not in diet_words:          #add all the species + data that didn't have blatant keywords (didn't get into diet_words) to m_diets\n",
    "        m_diets[item] = u_diets[item]  \n",
    "        \n",
    "#go through m_diets species data and look for diet words (the ones defined in the ad hoc lists above), keeping track of counts\n",
    "#pretty much exactly the same thing we did with diet_words\n",
    "m_diet_words = {}\n",
    "for species in m_diets:\n",
    "    for item in m_diets[species]:\n",
    "  #      print(item)\n",
    "        diet_found = False              #boolean to keep track of whether we found any diet words or not\n",
    "        for list_ in adhoc_diet_query:\n",
    "            for entry in list_:\n",
    "                if entry in item and \"prey of\" not in item:\n",
    "                    diet_found = True\n",
    "                    if species not in m_diet_words:\n",
    "                        m_diet_words[species] = {}\n",
    "                    if entry not in m_diet_words[species]:\n",
    "                        m_diet_words[species][entry] = 0\n",
    "                    m_diet_words[species][entry] += 1\n",
    "        #if we find a phrase like \"feed on\", it's usu vy specific, so just add the whole sentence to the dictionary rather than trying to find a specific diet word\n",
    "        if diet_found == False:\n",
    "            for phrase in keyphrases:\n",
    "                if phrase in item:\n",
    "                    if species not in m_diet_words:\n",
    "                        m_diet_words[species] = {}\n",
    "                    if item not in m_diet_words[species]:\n",
    "                        m_diet_words[species][item] = 0\n",
    "                    m_diet_words[species][item] += 1\n",
    "              #      print(entry, item)\n",
    "\n",
    "for species in m_diet_words:\n",
    "    print(species)\n",
    " #   print(m_diets[species])\n",
    "    print(m_diet_words[species])\n",
    "\n",
    "    \n",
    "#go through m_diet_words and diet_words and recategorize them into carnivore, herbivore, etc., with counts\n",
    "\n",
    "m_diet_categories = {}               #create a new dictionary to hold the categorization of the m_diets data\n",
    "for species in m_diet_words:\n",
    "    m_diet_categories[species] = {}  #add each species in m_diet _words to m_diet_categories\n",
    "  #  print(species)\n",
    "    for entry in m_diet_words[species]:              #for each diet word in m_diet_words\n",
    "   #     print(entry, m_diet_words[species][entry])\n",
    "        for list_ in searchable_diet_keywords:           #for each list of diets (carnivore, etc.)\n",
    "            if entry in searchable_diet_keywords[list_]:  #if the current diet word is in the current list\n",
    "                if list_ not in m_diet_categories[species]:  #if the list title (carnivore, herbivore, etc.) isn't already in m_diet_categories\n",
    "                    m_diet_categories[species][list_] = 0   #add the diet type to m_diet_categories\n",
    "                m_diet_categories[species][list_] += 1     #and keep track of how many times that category has appeared\n",
    "                #*****could also change this to add the m_diet_words count--so if insects shows up 3 times and plants 1, it reflects that****\n",
    "\n",
    "#for species that have counts of both carnivore and herbivore, class them as omnivores\n",
    "for species in m_diet_categories:\n",
    "    if \"carnivore\" in m_diet_categories[species] and \"herbivore\" in m_diet_categories[species]:\n",
    "        m_diet_categories[species]['omnivore'] = m_diet_categories[species]['carnivore'] + m_diet_categories[species]['herbivore']\n",
    "        del m_diet_categories[species]['carnivore']\n",
    "        del m_diet_categories[species]['herbivore']\n",
    "\n",
    "        \n",
    "#now do the same thing for diet_words\n",
    "u_diet_categories = {}\n",
    "for species in diet_words:\n",
    "    u_diet_categories[species] = {}\n",
    "    for entry in diet_words[species]:\n",
    "        for list_ in searchable_diet_keywords:\n",
    "            if entry in searchable_diet_keywords[list_]:\n",
    "                if list_ not in u_diet_categories[species]:\n",
    "                    u_diet_categories[species][list_] = 0\n",
    "                u_diet_categories[species][list_] += 1\n",
    "\n",
    "for species in u_diet_categories:\n",
    "    if \"carnivore\" in u_diet_categories[species] and \"herbivore\" in u_diet_categories[species]:\n",
    "        u_diet_categories[species]['omnivore'] = u_diet_categories[species]['carnivore'] + u_diet_categories[species]['herbivore']\n",
    "        del u_diet_categories[species]['carnivore']\n",
    "        del u_diet_categories[species]['herbivore']\n",
    "    print(species)\n",
    "    print(u_diet_categories[species])\n",
    "\n",
    "    \n",
    "#now that each species has a list of mentioned diets, put them all back into one dictionary\n",
    "#which means adding m_diet_categories back into u_diet_categories\n",
    "for entry in m_diet_categories:\n",
    "    if entry not in u_diet_categories:\n",
    "        u_diet_categories[entry] = m_diet_categories[entry]\n",
    "print(len(u_diet_categories))\n",
    "\n",
    "for entry in u_diet_categories:\n",
    "    try:\n",
    "        del u_diet_categories[entry]['likely diet']\n",
    "    except KeyError:\n",
    "        print('fine')\n",
    "    print(entry, u_diet_categories[entry])\n",
    "print(len(u_diet_categories))\n",
    "\n",
    "\n",
    "#okay, and now, the final step is to look at those entries that have multiple diet types and try to pick the most likely diet/\n",
    "#generalize to the highest level of the diet\n",
    "#this is done by working with the counts of the diet types we've collected\n",
    "u_diets_finished = {}\n",
    "for species in u_diet_categories:\n",
    "    print(\"\\n\")\n",
    "    print(species)\n",
    "    diets = []                       #list to hold the diets that were mentioned for the species\n",
    "    likely_diet = ('filler', 1)       #a filler that will replaced by the most common diet\n",
    "    threshold = 0\n",
    " #   print(species)\n",
    "    u_diets_finished[species] = {}\n",
    " #   print(u_diet_categories[species])\n",
    "    for entry in u_diet_categories[species]:      #for each diet type found for the species\n",
    "  #      print(entry, u_diet_categories[species][entry])\n",
    "        diets.append(entry)                       #add the diet type to 'diets'\n",
    "  #      print(type(u_diet_categories[species][entry]))\n",
    "\n",
    "        #if \"omnivore\" is in there, just assume it's an omnivore\n",
    "        if entry == 'omnivore':\n",
    "            u_diets_finished[species]['likely_diet'] = 'omnivore'\n",
    "            break\n",
    "            \n",
    "        #otherwise, if there's a value that's greater than the others, keep the greatest value\n",
    "        elif u_diet_categories[species][entry] > likely_diet[1]:\n",
    "            u_diets_finished[species]['likely_diet'] = entry\n",
    "            #threshold is the difference between the highest count and the previous highest count\n",
    "            threshold = likely_diet[1] - u_diet_categories[species][entry]\n",
    "            likely_diet = (entry, u_diet_categories[species][entry])\n",
    "    \n",
    "    #if multiple diets were found and the threshold of difference is less than 2\n",
    "    if len(diets) > 1 and threshold < 2 and threshold > -2:\n",
    "        print(diets)\n",
    "        print(threshold)\n",
    "     #   types = set()\n",
    "        types = []\n",
    "        eats = \"\"\n",
    "        for item in diets:                      #go through each mentioned diet type and see which broad categories were mentioned\n",
    "            for category in broad_categories:\n",
    "                if item in broad_categories[category]:\n",
    "        #            print(item)\n",
    "           #         types.add(category)\n",
    "                    types.append(category)\n",
    "                    eats = category\n",
    "        if len(types) > 1:                #if more than one broad diet type was mentioned (aka, if carnivore and herbivore were both mentioned)\n",
    "      #      print(species)\n",
    "            counter = Counter(types)            #keep track of number of times each element appears\n",
    "            print(counter)\n",
    "            print(types)\n",
    "            ordered = counter.most_common()   #put the counter in order of highest to lowest\n",
    "            highest = ordered[0]              #and get the highest number\n",
    "            try:\n",
    "                next_highest  = ordered[1]\n",
    "                if highest[1] - next_highest[1] >= 1:   #if highest diet type is 1 or higher than the next diet type\n",
    "                    likely_diet = highest[0]           #take highest element--since they're sorted into types, most common type is probably our best guess?\n",
    "                else:                                 #if it's tied, call it an omnivore\n",
    "                    likely_diet = \"omnivore\"\n",
    "            except IndexError:\n",
    "                likely_diet = highest[0]\n",
    "           \n",
    "            u_diets_finished[species]['likely_diet'] = likely_diet   #and add that categorization to the dictionary\n",
    "     #       print(likely_diet)\n",
    "        else:                                                        #if only one broad category was found, add that category to the dictionary\n",
    "            likely_diet = types\n",
    "            u_diets_finished[species]['likely_diet'] = likely_diet\n",
    "        print(likely_diet)\n",
    "      #      print(species)\n",
    "       #     print(u_diet_categories[species])\n",
    "        #    print(types)\n",
    "         #   print(likely_diet)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#now that we have updated diets in u_diets_finished, we can:\n",
    "    #read in the original .csv with the undefined diets\n",
    "    #go through and update the entries with the diet info in u_diets_finished\n",
    "    #read the updated dictionary into a new .csv file\n",
    "\n",
    "#read in original file\n",
    "\n",
    "cat_diets = {}     #dictionary to hold the species data (cat for soon-to-be-categorized)\n",
    "\n",
    "with open('allison_song_all_species_data.csv', 'r') as f:                 #update file name here\n",
    "    \n",
    "    next(f)  #skip the header\n",
    "    \n",
    "    reader = csv.reader(f)\n",
    "    count = 0\n",
    "    unfound = 0\n",
    "    \n",
    "    for row in reader:\n",
    "        try:\n",
    "            species = row[0]\n",
    "        except IndexError:\n",
    "            print(\"blank\")\n",
    "        if species not in cat_diets:  #add species to cat_diets\n",
    "            cat_diets[species] = []\n",
    "            \n",
    "        #find the undefined diets and, if a more specific diet was found, update the diet type\n",
    "   #     print(row)\n",
    "        if row[-1] == \"undefined\":\n",
    "            if species in u_diets_finished:\n",
    "                count += 1\n",
    "                print(species)\n",
    "                try:\n",
    "                    print(u_diets_finished[species]['likely_diet'])\n",
    "                    del row[-1]                                     #delete \"undefined\"\n",
    "                    diet = u_diets_finished[species]['likely_diet']\n",
    "                    row.append(diet)\n",
    "                except KeyError:\n",
    "                    print('remains undefined')\n",
    "                    unfound += 1\n",
    "            \n",
    "            cat_diets[species] = row         #put in remaining diet data (eol, wiki, etc.)\n",
    "        \n",
    "        else:      #if the diet is already defined, keep everything as is\n",
    "            cat_diets[species] = row\n",
    "    print(count)\n",
    "    print(unfound)\n",
    "            \n",
    "#last, write the updated data in cat_diets to a new .csv\n",
    "with open('allison_song_all_diets_categorized.csv', 'w') as f:            #update file name here\n",
    "    writer = csv.writer(f)\n",
    "    for species in cat_diets:\n",
    "        row = []\n",
    "        row.append(species)\n",
    "        for item in cat_diets[species]:\n",
    "            row.append(item)\n",
    "        writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtenv-py38",
   "language": "python",
   "name": "my-virtenv-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
