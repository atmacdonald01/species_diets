{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def run_species_diet(file_name):\n",
    "#when given file name, do all the work with it using the functions we have\n",
    "\n",
    "  species_list = species_dict(file_name)    #get the species names in a list\n",
    "  \n",
    "  #make a list/set of scientific and common names for use in our search functions; name is tuple of scientific name/common name######\n",
    "  species_list_non_dict = []\n",
    "  for item in species_list:\n",
    "    name = item['species_name']\n",
    "    #  scientific = item['scientific_name']\n",
    "     # common = item['common_name']\n",
    "      #name = (scientific, common)\n",
    "    \n",
    "    #avoid repeats\n",
    "    if name not in species_list_non_dict:\n",
    "        species_list_non_dict.append(name)\n",
    "\n",
    " #   print(len(species_list_non_dict))\n",
    "\n",
    "  ########split the big list into smaller ones so it's easier for the eol function to digest--trying to do thousands of species at once overloads the function###############\n",
    "  #produces an embedded list: the original list split into chunks 500 species long, and each chunk added to a list\n",
    "  small_lists = []\n",
    "  working_list = []\n",
    "  total_count = 0\n",
    "  count = 0\n",
    "  for item in species_list_non_dict:\n",
    "      working_list.append(item)\n",
    "      count += 1\n",
    "      total_count += 1\n",
    "      #if the list is 500 long, add that chunk to the master list and start a new chunk\n",
    "      if count == 500:\n",
    "          small_lists.append(working_list)\n",
    "          count = 0\n",
    "          working_list = []\n",
    "      elif total_count == len(species_list_non_dict):\n",
    "          small_lists.append(working_list)\n",
    "          \n",
    "  #check -- print the length of each chunk and the first item in that chunk\n",
    " # for item in small_lists:\n",
    "  #    print(len(item))\n",
    "   #   print(item[0])\n",
    "    #  print(\"\\n\")\n",
    "\n",
    "    ######use eol function on our species list##########\n",
    "  #using the check_eol function, create a dictionary of each species and the diet results found from scraping the EOL website\n",
    "  eol_species_diet = {}          #dictionary to hold our results\n",
    "  \n",
    "  #counts to measure results and where data is found or missing\n",
    "  count = 0\n",
    "  eats_unfound = 0\n",
    "  link_unfound = 0\n",
    "  species_unfound = 0\n",
    "  \n",
    "  #go through each mini-list; for each species, grab the scientific name and run the check_eol function on it\n",
    "  for list_ in small_lists:\n",
    "      count += 1\n",
    "      print(\"NOW CRUNCHING THROUGH LIST: \", count, \"\\n\")\n",
    "      eol_species_diet[count] = {}\n",
    "      for species in list_:\n",
    "          species = [species]\n",
    "          try:\n",
    "              scientific_name = species[0]\n",
    "          except IndexError:\n",
    "              print(\"unhashable species name\")\n",
    "       #   common_name = species[1]\n",
    "          eol_species_diet[count][scientific_name] = []\n",
    "          \n",
    "          try:\n",
    "              result = check_eol(scientific_name)\n",
    "\n",
    "              if result == \"No diet section and no article diet data.\":\n",
    "                  eats_unfound += 1\n",
    "\n",
    "              elif result == \"No species link found.\":\n",
    "                  link_unfound += 1\n",
    "\n",
    "              elif result == \"No results found.\":\n",
    "                  species_unfound += 1\n",
    "\n",
    "              else:\n",
    "                  eol_species_diet[count][scientific_name] = result\n",
    "        #          print(species, result)\n",
    "            \n",
    "          except requests.exceptions.ConnectionError:\n",
    "                print(species, \"CONNECTION FAILED\")\n",
    "              \n",
    " # print(eats_unfound)\n",
    "  #print(link_unfound)\n",
    "  \n",
    "#eol_species_diet now has all the eol data in it \n",
    "\n",
    "##########use wikipedia function on our species list##############\n",
    "  #search wikipedia for diets\n",
    "  wiki_species_diet = {}         #dictionary to hold species and the diets that are found\n",
    "  \n",
    "  unfound = 0\n",
    "  no_keywords = []\n",
    "  \n",
    "  #look through each species in our species list\n",
    "  for species in species_list_non_dict:\n",
    "      \n",
    "      try:\n",
    "          diet = check_wiki(species)                 #run check_wiki on the species\n",
    "\n",
    "          #if unsuccessful, try genus name\n",
    "          if diet == None:\n",
    "              genus = species.split()[0]\n",
    "              diet = check_wiki(genus)\n",
    "\n",
    "          #if still unsuccessful, count as unfound\n",
    "          if diet == None:\n",
    "              unfound += 1\n",
    "              no_keywords.append(species)\n",
    "                \n",
    "      except requests.exceptions.ConnectionError:\n",
    "        print(species, \"CONNECTION FAILED\")\n",
    "\n",
    "    #add diet to wiki_diets dictionary\n",
    "      wiki_species_diet[species] = diet\n",
    "          \n",
    "    #  print(diet)\n",
    "      \n",
    " # print(unfound)\n",
    "  #print(no_keywords)\n",
    "  #print(wiki_species_diet)\n",
    "#  for item in wiki_species_diet:\n",
    " #   print(item)\n",
    "  #  print(wiki_species_diet[item])\n",
    "    \n",
    "    \n",
    "  #wiki_species_diet and eol_species_diet now hold the wikipedia and eol data respectively\n",
    "  \n",
    "  ########consolidate wiki_species_diet using the consolidate function#########\n",
    "  consolidated_wiki_diets = consolidate(wiki_species_diet)\n",
    " # for entry in consolidated_wiki_diets:\n",
    "  #  print(entry)\n",
    "   # print(consolidated_wiki_diets[entry])\n",
    "  \n",
    "  #consolidated_wiki_diets and eol_species_diet now hold the wikipedia and eol data\n",
    "  \n",
    "  #####COMBINE THE EOL AND WIKIPEDIA DATA###########\n",
    "  all_diets = {}\n",
    "  for species in species_list_non_dict:\n",
    "      #make an entry for each species we have with slots for wikipedia data, eol data, and a diet guess\n",
    "      try:\n",
    "   #       species = species[0]\n",
    "     #     print(species)\n",
    "          all_diets[species] = {}\n",
    "          all_diets[species]['wikipedia'] = []\n",
    "          all_diets[species]['eol'] = []\n",
    "          all_diets[species]['diet_guess'] = \"\"\n",
    "      except IndexError:\n",
    "          print(\"blank\")\n",
    "  \n",
    "  #go through the wikipedia diets and add the data to all_diets\n",
    "  for species in consolidated_wiki_diets:\n",
    "      species = [species]            #make species a list so the indexing works\n",
    "    #  all_diets[species[0]]['wikipedia'] = []\n",
    "      #add the diet sentences we found for each diet type\n",
    "      for entry in consolidated_wiki_diets[species[0]]:\n",
    "            if entry != \"most_likely\":\n",
    "                all_diets[species[0]]['wikipedia'].append(consolidated_wiki_diets[species[0]][entry]['sentences'])\n",
    "      #if there's no diet guess for this species, fill in the wikipedia diet guess\n",
    "      if all_diets[species[0]]['diet_guess'] == \"\":\n",
    "          all_diets[species[0]]['diet_guess'] = consolidated_wiki_diets[species[0]]['most_likely']\n",
    "  \n",
    "  #go through the eol diets and add the data to all_diets\n",
    "#  for species in eol_species_diet:\n",
    " #     all_diets[species]['eol'] = eol_species_diet[species]['diet']\n",
    "  \n",
    "  for item in eol_species_diet:\n",
    "   #     print(item)\n",
    "    #    print(eol_species_diet[item])\n",
    "        for species in eol_species_diet[count]:\n",
    "     #       print(species, eol_species_diet[count][species])\n",
    "            all_diets[species]['eol'] = eol_species_diet[count][species]\n",
    "  \n",
    "\n",
    "  #####store consolidated data in a .csv##########\n",
    "  #write the data we have to a .csv\n",
    "  name = file_name + 'all_species_data.csv'\n",
    "  with open(name, 'w') as f:\n",
    "      writer = csv.writer(f)\n",
    "      \n",
    "      header = (\"species name\", \"Wikipedia data\", \"EOL data\", \"diet guess\")\n",
    "      writer.writerow(header)\n",
    "      \n",
    "      for entry in all_diets:\n",
    "          row = \"\"\n",
    "          species = entry\n",
    "          wiki = all_diets[entry]['wikipedia']\n",
    "          eol = all_diets[entry]['eol']\n",
    "          guess = all_diets[entry]['diet_guess']\n",
    "          row = (species, wiki, eol, guess)\n",
    "          writer.writerow(row)\n",
    "      \n",
    "    \n",
    "  \n",
    "  ########POST PROCESSING###############\n",
    "  #finalize diet guesses and examine the data more thoroughly to try to categorize \"undefined\" diets\n",
    "  \n",
    "  u_diets = {}         #dictionary to hold all the species with an \"undefined\" diet type\n",
    "  \n",
    "  with open(name, 'r') as f:     #open the file with the EOL/wikipedia data and the diet guesses for each species\n",
    "      \n",
    "      next(f)    #skip the header\n",
    "      \n",
    "      reader = csv.reader(f)\n",
    "      \n",
    "      for row in reader:\n",
    "          #filter out the undefined diets\n",
    "          if row[-1] == \"undefined\":\n",
    "    #          print(row)\n",
    "              species = row[0]\n",
    "              u_diets[species] = []               #create an entry for the undefined species\n",
    "              if row[1] != []:                    #and add the EOL and wikipedia data, if it's there\n",
    "                  u_diets[species].append(row[1])\n",
    "              if row[2] != []:\n",
    "                  u_diets[species].append(row[2])\n",
    "  \n",
    "  \n",
    "                  \n",
    "  #####first, look through the entries for species that have one of the diet categories listed above. These will be stored in \"diet_words\"\n",
    "  #Also record how many times each diet category is mentioned--many of these undefined species fall under multiple categories########\n",
    "  \n",
    "  diet_words = {}\n",
    "  \n",
    "  #go through the species data and look for existing keywords\n",
    "  for species in u_diets:\n",
    "  #    print(species)\n",
    "      for item in u_diets[species]:    #for each dataset (EOL, wikipedia) for each species\n",
    "     #     print(item)\n",
    "          for entry in all_diet_query:   #for the lists of each diet category (carnivore, herbivore, etc.)\n",
    "       #      print(entry) \n",
    "             if entry in item.lower():          #if that diet word is in the EOL or wikipedia dataset\n",
    "             #    print(entry)\n",
    "                 if species not in diet_words:\n",
    "                     diet_words[species] = {}    #give that species an entry in diet_words\n",
    "                #      print(entry, '\\n')\n",
    "                 if entry not in diet_words[species]:  #and add the diet word and its count to the species in diet_words\n",
    "                     diet_words[species][entry] = 0\n",
    "                 diet_words[species][entry] += 1\n",
    "                    \n",
    " # print(len(diet_words))\n",
    "  \n",
    "     \n",
    "  \n",
    "  ########the next step is to look at the entries that don't have those convenient keywords and use a more specialized list to try to define their diet type.#####\n",
    "  \n",
    "  m_diets = {}            #m_diets for mystery diets\n",
    "  for item in u_diets:\n",
    "      if item not in diet_words:          #add all the species + data that didn't have blatant keywords (didn't get into diet_words) to m_diets\n",
    "          m_diets[item] = u_diets[item]  \n",
    "  \n",
    " # print(len(m_diets))       \n",
    "          \n",
    "  ######definining new sets of keywords########\n",
    "  adhoc_carnivore = [\"fish\", \"meat\", \"invertebrates\", \"insects\", \"crabs\", \"crustaceans\", \"snake\", \"mammal\", \"frog\", \"bird\", \"arthropod\", \"caterpillar\", \"beetle\", \"prey\"]\n",
    "  adhoc_herbivore = [\"fruit\", \"seed\", \"flower\", \"leaves\", \"leaf,\", \"plant\", \"wood\", \"nectar\", \"tree\", \"shrub\", \"twig\", \"foliage\", \"bark\", \"berries\", \"leaf litter\", \"pollen\", \"seeds\", \"algae\", \"mosses\", \"sap\", \"brassicas\", \"spurge\", \"lichens\", \"iguana\", \"tortoise\"]\n",
    "  adhoc_omnivore = [\"eggs\", \"fungi\"]\n",
    "  adhoc_scavenger = [\"dung\", \"feces\", \"faeces\"]\n",
    "  adhoc_diet_query = (adhoc_carnivore, adhoc_herbivore, adhoc_omnivore, adhoc_scavenger)\n",
    "  keyphrases = [\"feed on\", \"feeds on\", \"feeding on\"]\n",
    "  \n",
    "  \n",
    "  \n",
    "  ###############go through m_diets species data and look for diet words (the ones defined in the ad hoc lists above), keeping track of counts##########\n",
    "  #pretty much exactly the same thing we did with diet_words\n",
    "  m_diet_words = {}\n",
    "  for species in m_diets:\n",
    "      for item in m_diets[species]:\n",
    "    #      print(item)\n",
    "          diet_found = False              #boolean to keep track of whether we found any diet words or not\n",
    "          for list_ in adhoc_diet_query:\n",
    "              for entry in list_:\n",
    "                  if entry in item and \"prey of\" not in item:\n",
    "                      diet_found = True\n",
    "                      if species not in m_diet_words:\n",
    "                          m_diet_words[species] = {}\n",
    "                      if entry not in m_diet_words[species]:\n",
    "                          m_diet_words[species][entry] = 0\n",
    "                      m_diet_words[species][entry] += 1\n",
    "          #if we find a phrase like \"feed on\", it's usu vy specific, so just add the whole sentence to the dictionary rather than trying to find a specific diet word\n",
    "          if diet_found == False:\n",
    "              for phrase in keyphrases:\n",
    "                  if phrase in item:\n",
    "                      if species not in m_diet_words:\n",
    "                          m_diet_words[species] = {}\n",
    "                      if item not in m_diet_words[species]:\n",
    "                          m_diet_words[species][item] = 0\n",
    "                      m_diet_words[species][item] += 1\n",
    "                #      print(entry, item)\n",
    "  \n",
    " # for species in m_diet_words:\n",
    "  #    print(species)\n",
    "   #   print(m_diet_words[species])\n",
    "  \n",
    "  \n",
    "      \n",
    "  ##########create the inverse of the ad hoc lists so that we can reverse-categorize species in m_diet_words based on what specific foods they were found to consume#########\n",
    "  searchable_diet_keywords = {}\n",
    "  searchable_diet_keywords['carnivore'] = [\"fish\", \"meat\", \"invertebrates\", \"insects\", \"crabs\", \"crustaceans\", \"snake\", \"mammal\", \"frog\", \"bird\", \"arthropod\", \"caterpillar\", \"beetle\", \"prey\", \"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod']\n",
    "  searchable_diet_keywords['herbivore'] = [\"fruit\", \"seed\", \"flower\", \"leaves\", \"leaf,\", \"plant\", \"wood\", \"nectar\", \"tree\", \"shrub\", \"twig\", \"foliage\", \"bark\", \"berries\", \"leaf litter\", \"pollen\", \"seeds\", \"algae\", \"mosses\", \"sap\", \"brassicas\", \"spurge\", \"lichens\", \"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'tortoise', 'iguana']\n",
    "  searchable_diet_keywords['omnivore'] = [\"eggs\", \"fungi\", \"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\"]\n",
    "  searchable_diet_keywords['scavenger'] = [\"dung\", \"feces\", \"faeces\", \"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous']\n",
    "  searchable_diet_keywords['insectivore'] = [\"insectivore\", \"Insectivore\", 'insectivorous', \"Insectivorous\", 'entomophagous', 'spider']\n",
    "  searchable_diet_keywords['frugivore'] = [\"frugivore\", \"Frugivore\", 'frugivorous', 'Frugivorous']\n",
    "  searchable_diet_keywords['bloodfeeder'] = [\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\"]\n",
    "  searchable_diet_keywords['granivore'] = [\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\"]\n",
    "  searchable_diet_keywords['nectarivore'] = [\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\"]\n",
    "  searchable_diet_keywords['folivore'] = [\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\"]\n",
    "  searchable_diet_keywords['gummivore'] = [\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\"]\n",
    "  searchable_diet_keywords['filterfeeder'] = [\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ']\n",
    "      \n",
    "      \n",
    "\n",
    "  ################go through m_diet_words and diet_words and recategorize them into carnivore, herbivore, etc., with counts###############\n",
    "  \n",
    "  m_diet_categories = {}               #create a new dictionary to hold the categorization of the m_diets data\n",
    "  for species in m_diet_words:\n",
    "      m_diet_categories[species] = {}  #add each species in m_diet _words to m_diet_categories\n",
    "    #  print(species)\n",
    "      for entry in m_diet_words[species]:              #for each diet word in m_diet_words\n",
    "     #     print(entry, m_diet_words[species][entry])\n",
    "          for list_ in searchable_diet_keywords:           #for each list of diets (carnivore, etc.)\n",
    "              if entry in searchable_diet_keywords[list_]:  #if the current diet word is in the current list\n",
    "                  if list_ not in m_diet_categories[species]:  #if the list title (carnivore, herbivore, etc.) isn't already in m_diet_categories\n",
    "                      m_diet_categories[species][list_] = 0   #add the diet type to m_diet_categories\n",
    "                  m_diet_categories[species][list_] += 1     #and keep track of how many times that category has appeared\n",
    "                  #*****could also change this to add the m_diet_words count--so if insects shows up 3 times and plants 1, it reflects that****\n",
    "  \n",
    "  #for species that have counts of both carnivore and herbivore, class them as omnivores\n",
    "  for species in m_diet_categories:\n",
    "      if \"carnivore\" in m_diet_categories[species] and \"herbivore\" in m_diet_categories[species]:\n",
    "          m_diet_categories[species]['omnivore'] = m_diet_categories[species]['carnivore'] + m_diet_categories[species]['herbivore']\n",
    "          del m_diet_categories[species]['carnivore']\n",
    "          del m_diet_categories[species]['herbivore']\n",
    "  \n",
    "          \n",
    "  #now do the same thing for diet_words\n",
    "  u_diet_categories = {}\n",
    "  for species in diet_words:\n",
    "      u_diet_categories[species] = {}\n",
    "      for entry in diet_words[species]:\n",
    "          for list_ in searchable_diet_keywords:\n",
    "              if entry in searchable_diet_keywords[list_]:\n",
    "                  if list_ not in u_diet_categories[species]:\n",
    "                      u_diet_categories[species][list_] = 0\n",
    "                  u_diet_categories[species][list_] += 1\n",
    " # print(u_diet_categories)\n",
    "  for species in u_diet_categories:\n",
    "      if \"carnivore\" in u_diet_categories[species] and \"herbivore\" in u_diet_categories[species]:\n",
    "          u_diet_categories[species]['omnivore'] = u_diet_categories[species]['carnivore'] + u_diet_categories[species]['herbivore']\n",
    "          del u_diet_categories[species]['carnivore']\n",
    "          del u_diet_categories[species]['herbivore']\n",
    "   #   print(species)\n",
    "    #  print(u_diet_categories[species])\n",
    "  \n",
    "      \n",
    "      \n",
    "  ###############now that each species has a list of mentioned diets, put them all back into one dictionary###############\n",
    "  #which means adding m_diet_categories back into u_diet_categories\n",
    "  for entry in m_diet_categories:\n",
    "      if entry not in u_diet_categories:\n",
    "          u_diet_categories[entry] = m_diet_categories[entry]\n",
    "#  print(len(u_diet_categories))\n",
    "      \n",
    "      \n",
    "  for entry in u_diet_categories:\n",
    "      try:\n",
    "          del u_diet_categories[entry]['likely diet']\n",
    "      except KeyError:\n",
    "          print('fine')\n",
    " #     print(entry, u_diet_categories[entry])\n",
    "  #print(len(u_diet_categories))\n",
    "  \n",
    "  \n",
    "  \n",
    "  ############define broad categories--so e.g. something that's both a frugivore and a folivore is an herbivore###########\n",
    "  broad_herbivore = ['frugivore', 'herbivore', 'granivore', 'nectarivore', 'folivore']\n",
    "  broad_omnivore = ['omnivore', 'gummivore', 'filter feeder']\n",
    "  broad_carnivore = ['carnivore', 'insectivore', 'bloodfeeder', 'scavenger']\n",
    "  \n",
    "  #for ease of use later, combine these broad categories into a dictionary\n",
    "  broad_categories = {}\n",
    "  broad_categories['herbivore'] = broad_herbivore\n",
    "  broad_categories['omnivore'] = broad_omnivore\n",
    "  broad_categories['carnivore'] = broad_carnivore\n",
    "  \n",
    "#  print(broad_categories)\n",
    "      \n",
    "      \n",
    "  \n",
    "  ###########now, the final step is to look at those entries that have multiple diet types and try to pick the most likely diet/generalize to the highest level of the diet###########\n",
    "  #this is done by working with the counts of the diet types we've collected\n",
    "  u_diets_finished = {}\n",
    "  for species in u_diet_categories:\n",
    "  #    print(\"\\n\")\n",
    "   #   print(species)\n",
    "      diets = []                       #list to hold the diets that were mentioned for the species\n",
    "      likely_diet = ('filler', 1)       #a filler that will replaced by the most common diet\n",
    "      threshold = 0\n",
    "   #   print(species)\n",
    "      u_diets_finished[species] = {}\n",
    "   #   print(u_diet_categories[species])\n",
    "      for entry in u_diet_categories[species]:      #for each diet type found for the species\n",
    "    #      print(entry, u_diet_categories[species][entry])\n",
    "          diets.append(entry)                       #add the diet type to 'diets'\n",
    "    #      print(type(u_diet_categories[species][entry]))\n",
    "  \n",
    "          #if \"omnivore\" is in there, just assume it's an omnivore\n",
    "          if entry == 'omnivore':\n",
    "              u_diets_finished[species]['likely_diet'] = 'omnivore'\n",
    "              break\n",
    "              \n",
    "          #otherwise, if there's a value that's greater than the others, keep the greatest value\n",
    "          elif u_diet_categories[species][entry] > likely_diet[1]:\n",
    "              u_diets_finished[species]['likely_diet'] = entry\n",
    "              #threshold is the difference between the highest count and the previous highest count\n",
    "              threshold = likely_diet[1] - u_diet_categories[species][entry]\n",
    "              likely_diet = (entry, u_diet_categories[species][entry])\n",
    "      \n",
    "      #if multiple diets were found and the threshold of difference is less than 2\n",
    "      if len(diets) > 1 and threshold < 2 and threshold > -2:\n",
    "       #   print(diets)\n",
    "        #  print(threshold)\n",
    "       #   types = set()\n",
    "          types = []\n",
    "          eats = \"\"\n",
    "          for item in diets:                      #go through each mentioned diet type and see which broad categories were mentioned\n",
    "              for category in broad_categories:\n",
    "                  if item in broad_categories[category]:\n",
    "          #            print(item)\n",
    "             #         types.add(category)\n",
    "                      types.append(category)\n",
    "                      eats = category\n",
    "          if len(types) > 1:                #if more than one broad diet type was mentioned (aka, if carnivore and herbivore were both mentioned)\n",
    "        #      print(species)\n",
    "              counter = Counter(types)            #keep track of number of times each element appears\n",
    "       #       print(counter)\n",
    "        #      print(types)\n",
    "              ordered = counter.most_common()   #put the counter in order of highest to lowest\n",
    "              highest = ordered[0]              #and get the highest number\n",
    "              try:\n",
    "                  next_highest  = ordered[1]\n",
    "                  if highest[1] - next_highest[1] >= 1:   #if highest diet type is 1 or higher than the next diet type\n",
    "                      likely_diet = highest[0]           #take highest element--since they're sorted into types, most common type is probably our best guess?\n",
    "                  else:                                 #if it's tied, call it an omnivore\n",
    "                      likely_diet = \"omnivore\"\n",
    "              except IndexError:\n",
    "                  likely_diet = highest[0]\n",
    "             \n",
    "              u_diets_finished[species]['likely_diet'] = likely_diet   #and add that categorization to the dictionary\n",
    "       #       print(likely_diet)\n",
    "          else:                                                        #if only one broad category was found, add that category to the dictionary\n",
    "              likely_diet = types\n",
    "              u_diets_finished[species]['likely_diet'] = likely_diet\n",
    "      #    print(likely_diet)\n",
    "        #      print(species)\n",
    "         #     print(u_diet_categories[species])\n",
    "          #    print(types)\n",
    "           #   print(likely_diet)\n",
    "   \n",
    "\n",
    "  ############finalizing and storing data################### \n",
    "  #now that we have updated diets in u_diets_finished, we can:\n",
    "      #read in the original .csv with the undefined diets\n",
    "      #go through and update the entries with the diet info in u_diets_finished\n",
    "      #read the updated dictionary into a new .csv file\n",
    "  \n",
    "  #read in original file\n",
    "  \n",
    "  cat_diets = {}     #dictionary to hold the species data (cat for soon-to-be-categorized)\n",
    "  \n",
    "  with open(name, 'r') as f:\n",
    "      \n",
    "      next(f)  #skip the header\n",
    "      \n",
    "      reader = csv.reader(f)\n",
    "      count = 0\n",
    "      unfound = 0\n",
    "      \n",
    "      for row in reader:\n",
    "          try:\n",
    "              species = row[0]\n",
    "          except IndexError:\n",
    "              print(\"blank\")\n",
    "          if species not in cat_diets:  #add species to cat_diets\n",
    "              cat_diets[species] = []\n",
    "              \n",
    "          #find the undefined diets and, if a more specific diet was found, update the diet type\n",
    "     #     print(row)\n",
    "          if row[-1] == \"undefined\":\n",
    "              if species in u_diets_finished:\n",
    "                  count += 1\n",
    "             #     print(species)\n",
    "                  try:\n",
    "               #       print(u_diets_finished[species]['likely_diet'])\n",
    "                      del row[-1]                                     #delete \"undefined\"\n",
    "                      diet = u_diets_finished[species]['likely_diet']\n",
    "                      row.append(diet)\n",
    "                  except KeyError:\n",
    "                      print('remains undefined')\n",
    "                      unfound += 1\n",
    "              \n",
    "              cat_diets[species] = row         #put in remaining diet data (eol, wiki, etc.)\n",
    "          \n",
    "          else:      #if the diet is already defined, keep everything as is\n",
    "              cat_diets[species] = row\n",
    "   #   print(count)\n",
    "    #  print(unfound)\n",
    "              \n",
    "      \n",
    "  #last, write the updated data in cat_diets to a new .csv\n",
    "  final_name = file_name + \"all_diets_categorized.csv\"\n",
    "  with open(final_name, 'w') as f:\n",
    "      writer = csv.writer(f)\n",
    "      for species in cat_diets:\n",
    "          row = []\n",
    "          row.append(species)\n",
    "          for item in cat_diets[species]:\n",
    "              row.append(item)\n",
    "          writer.writerow(row)\n",
    "  \n",
    "      \n",
    "\n",
    "  return(final_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####define diet categories####\n",
    "\n",
    "carnivore = [\"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod'] # 'predator']  #how to deal with words like \"predator\" that are used in contradictory contexts?\n",
    "herbivore = [\"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'tortoise', 'iguana']\n",
    "omnivore = [\"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\", 'omnivores']\n",
    "insectivore = [\"insectivore\", \"Insectivore\", 'insectivorous', \"Insectivorous\", 'entomophagous', 'spider']\n",
    "frugivore = [\"frugivore\", \"Frugivore\", 'frugivorous', 'Frugivorous']\n",
    "bloodfeeder = [\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\"]\n",
    "scavenger = [\"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous']\n",
    "granivore = [\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\"]\n",
    "nectarivore = [\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\"]\n",
    "folivore = [\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\"]\n",
    "gummivore = [\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\"]\n",
    "filterfeeder = [\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ']\n",
    "#and a category for food-related words that aren't specific to one diet\n",
    "undefined = [' eat', ' food ', ' consume', ' feed', ' diet ', ' prey']\n",
    "\n",
    "#make a master list of all query strings\n",
    "all_diet_query=carnivore + herbivore + omnivore + insectivore + frugivore + bloodfeeder + scavenger + granivore + nectarivore + folivore + gummivore + filterfeeder + undefined\n",
    "\n",
    "\n",
    "#######read in initial species list###########\n",
    "\n",
    "#a function that takes a .csv file with species data and returns that data as a dictionary\n",
    "#values can be adjusted depending on how csv is organized\n",
    "\n",
    "def species_dict(file):\n",
    "    \n",
    "    #a list to hold the species in the file\n",
    "    species_list = []\n",
    "    \n",
    "    #open the file and read the values into our \n",
    "    with open(file, \"r\") as metadata:\n",
    "        \n",
    "        #skip the first line, since that's just the title line\n",
    "        next(metadata)\n",
    "        \n",
    "        csvreader = csv.reader(metadata)\n",
    "        \n",
    "        #for each row, make a dictionary for the species\n",
    "        for row in csvreader:\n",
    "            if len(row) > 0:\n",
    "                species = {}\n",
    "                species['species_name'] = row[0].strip()    #values can be adjusted depending on layout of file being read\n",
    "            #    species['common_name'] = row[1].strip()\n",
    "             #   species['class_name'] = row[2].strip()\n",
    "\n",
    "                #and add it to the species_list, avoiding repeats\n",
    "                if species not in species_list:\n",
    "                    species_list.append(species)\n",
    "            \n",
    "    return species_list\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######EOL SEARCH FUNCTION##########\n",
    "#using html/BeautifulSoup, scrape the Encyclopedia of Life website for diet data\n",
    "#this function looks up the species in EOL, checks the \"eat\" section of its page, looks at the species overview, and looks at\n",
    "#its \"data\" section for information about its data\n",
    "\n",
    "def check_eol(species):\n",
    "    diet = []                        #establish diet list\n",
    "    og_species = species.lower()     #and species name\n",
    "    \n",
    "    no_eats = False\n",
    "    \n",
    "    #first, build the search URL from the species name\n",
    "    species = species.lower().split(\" \")\n",
    "    species = \"+\".join(species)\n",
    "    url = \"https://eol.org/search?utf8=%E2%9C%93&q=\" + species\n",
    "    \n",
    "    #go to the url\n",
    "    page = requests.get(url)      #use 'requests' library to grab the html code of the url\n",
    "    #and get the html\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")      #get the html content of the page using BeautifulSoup\n",
    "    \n",
    "    #get the list of search results\n",
    "    search = soup.find(class_=\"search-results js-search-results\")                    #pick out the results section from the html\n",
    "    if search != None:\n",
    "        results_list = search.find_all(\"a\")                                             #pick out the list of search results\n",
    "    \n",
    "    \n",
    "        #find the result with the most associated media\n",
    "        highest = 0\n",
    "        choice = \"\"\n",
    "        species_link = \"\"\n",
    "        for item in results_list:                                        #for each result, find the species name and how many articles are associated with it\n",
    "            link = item['href']                                        #get the link for the result\n",
    "            result = item.find(\"div\", class_=\"search-title\").text       #get the search title (the text of the result)\n",
    "            if og_species in result.lower():                           #if the result has the correct species name, look at it\n",
    "                media = item.find(\"ul\", class_=\"resource-bubbles\")\n",
    "                total_media = media.text.split(\"MEDIA\")[0].strip()            #split off the first number (the # of associated media) from other numbers\n",
    "                if total_media.isdigit() and int(total_media) > highest:                                   #if this number is a true number (not \\n or '1 MEDIUM', for ex)\n",
    "                    highest = int(total_media)\n",
    "                    choice = item\n",
    "                    species_link = \"http://eol.org\" + link #+ \"/data\"         #get the address of the page with the highest results and go to the \"data\" section\n",
    "            \n",
    "        #now that we have a species page, we need to check the overview, the \"data\" page, and the \"articles\" page for diet information\n",
    "        if len(species_link) > 0:\n",
    "            \n",
    "            #####\n",
    "            #the first step is to go to the overview page and see if there's a diet category there\n",
    "            general_page = requests.get(species_link)\n",
    "            general_soup = BeautifulSoup(general_page.content, \"html.parser\")\n",
    "            general = general_soup.find(\"p\")\n",
    "            try:\n",
    "                general_info = general.text.lower()\n",
    "                for item in all_diet_query:\n",
    "                    #if we find a diet type, get the sentence and add it to \"diet\"\n",
    "                    if item in general_info:\n",
    "                        sentence = \"\"\n",
    "                        index = general_info.index(item)\n",
    "                        stop = False\n",
    "                        #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                        while stop == False:\n",
    "                            try:\n",
    "                                sentence += general_info[index]\n",
    "                            except IndexError:\n",
    "                                break\n",
    "                            index += 1\n",
    "                            try:\n",
    "                                if general_info[index] == \".\":\n",
    "                                    stop = True\n",
    "                            except IndexError:\n",
    "                                break\n",
    "                        diet.append(sentence)             #add the sentence to the diet list\n",
    "            except AttributeError:\n",
    "                print(\"No overview found.\")\n",
    "\n",
    "            ######\n",
    "            #the next step is to go to the \"data\" page and see if there's an \"eat\" section, so we'll get the html content of the species data page\n",
    "            species_data_link = species_link + \"/data\"\n",
    "            data_page = requests.get(species_data_link)\n",
    "            data_soup = BeautifulSoup(data_page.content, \"html.parser\")\n",
    "        \n",
    "            #find the \"eat\" category out of the \"data\" section on the species page\n",
    "            categories = data_soup.find_all(\"a\", class_=\"item\")             #find all the data categories on the \"data\" page of the species\n",
    "            diet_link = \"\"\n",
    "            for category in categories:\n",
    "                if category.text == \"eat\":\n",
    "                    diet_link = \"http://eol.org\" + category['href']             #if there's an \"eat\" section, get the diet page url\n",
    "    \n",
    "            #if there was an \"eat\" section, get the html content of the diet page\n",
    "            if len(diet_link) > 0:\n",
    "                diet_page = requests.get(diet_link)\n",
    "                diet_soup = BeautifulSoup(diet_page.content, \"html.parser\")\n",
    "    \n",
    "                #add each eaten species to the diet list\n",
    "                eats = diet_soup.find_all(\"div\", class_=\"trait-val\")       #find the listed species that species eats\n",
    "                for eat in eats:\n",
    "                    diet.append(eat.text.strip())\n",
    "        \n",
    "            #if there's no \"eat\" section, say so\n",
    "            else:\n",
    "                print(\"No eats found.\")\n",
    "                no_eats = True\n",
    "            \n",
    "            ########\n",
    "            #if diet is still empty, the next step is to check the \"articles\" section for diet information\n",
    "            if no_eats == True and diet == []:\n",
    "                species_article_link = species_link + \"/articles\"\n",
    "                article_page = requests.get(species_article_link)\n",
    "                article_soup = BeautifulSoup(article_page.content, \"html.parser\")\n",
    "\n",
    "                #now that we have the html of the \"articles\" page, we need to get its text and look through it for diet keywords\n",
    "                article_text = []                                     # a list to hold the text of the articles\n",
    "                articles = article_soup.find_all(\"div\", class_=\"body uk-margin-small-top\")        #get the article sections\n",
    "                #and add each article to the list of article_text\n",
    "                for article in articles:\n",
    "                    article_text.append(article.text)\n",
    "\n",
    "                #then, go through each item in article_text and look for diet information\n",
    "                for a in article_text:\n",
    "                    for item in all_diet_query:                #loop through our list of diet types and see if they're mentioned in the page\n",
    "                        #if we find a diet type in the article\n",
    "                        if item in a:\n",
    "                            sentence = \"\"\n",
    "                            index = a.index(item)\n",
    "                            stop = False\n",
    "                            #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                            while stop == False:\n",
    "                                try:\n",
    "                                    sentence += a[index]\n",
    "                                except IndexError:\n",
    "                                    break\n",
    "                                index += 1\n",
    "                                try:\n",
    "                                    if a[index] == \".\":\n",
    "                                        stop = True\n",
    "                                except IndexError:\n",
    "                                    break\n",
    "                            diet.append(sentence)             #add the sentence to the diet list\n",
    "\n",
    "            ######\n",
    "            #if diet is still empty, try looking up the classification tree\n",
    "            tree = general_soup.find(\"div\", class_=\"hier js-hier-summary\")\n",
    "            try:\n",
    "                branches = tree.find_all(\"a\")\n",
    "                #look at each step in the tree and add it to diet if it matches a diet type (amphibians, carnivores, etc.)\n",
    "                for branch in branches:\n",
    "                    if branch.text.lower() in all_diet_query:\n",
    "                        diet.append(branch.text.lower())\n",
    "            except AttributeError:\n",
    "                print(\"No tree found.\")\n",
    "            \n",
    "            \n",
    "            #######                \n",
    "            #if we go through all that and still have no diet info, say so\n",
    "            if no_eats == True and diet == []:\n",
    "                print(\"No diet section and no article diet data.\")\n",
    "                diet = \"No diet section and no article diet data.\"\n",
    "        \n",
    "        #if there's no species link, say so\n",
    "        else:\n",
    "            print(\"No species link found.\") \n",
    "            diet = \"No species link found.\"\n",
    "        \n",
    "    #if there's no results page, say so\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "        diet = \"No results found.\"\n",
    "        \n",
    "    return diet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "##########WIKIPEDIA SEARCH FUNCTION###############\n",
    "#a function to search wikipedia: takes a species name and returns a dictionary of its diet data\n",
    "def check_wiki(species):\n",
    "    species = species.strip()   #clean species name of any extraneous spaces\n",
    "    \n",
    "    #establish dictionary and count of how many diet-type instances found\n",
    "    diet = {}\n",
    "    total_count = 0\n",
    "    \n",
    "    #look for the species in wikipedia\n",
    "    try:\n",
    "        page = wikipedia.page(species, auto_suggest=False)                 #get page, if possible\n",
    "        page_content = page.content                                        #pull the content of the page\n",
    "        for item in all_diet_query:                #loop through our list of diet types and see if they're mentioned in the page\n",
    "            #if we find a diet type in page_content\n",
    "            if item in page_content:\n",
    "                #make an entry for each diet type that is found\n",
    "                if item not in diet.keys():        \n",
    "                    diet[item] = {}    \n",
    "                    diet[item][\"count\"] = 0         \n",
    "                    diet[item][\"mentions\"] = []\n",
    "                    \n",
    "                #add diet type to \"diet\" dictionary with spots for counts of each diet type mentioned and the sentences the mention occurs in\n",
    "                index = page_content.index(item)\n",
    "                sentence = \"\"\n",
    "                stop = False\n",
    "                #add the sentence up to the next period to get the full sentence from where diet was mentioned\n",
    "                while stop == False:\n",
    "                    try:\n",
    "                        sentence += page_content[index]\n",
    "                    except IndexError:\n",
    "                        break\n",
    "                    index += 1\n",
    "                    try:\n",
    "                        if page_content[index] == \".\":\n",
    "                            stop = True\n",
    "                    except IndexError:\n",
    "                        break\n",
    "                \n",
    "                #increase the count of the diet that was found and append the sentence to the dictionary\n",
    "                diet[item][\"count\"] += 1\n",
    "                diet[item][\"mentions\"].append(sentence)\n",
    "                \n",
    "                total_count += 1                   #count up that we've found a diet type\n",
    "                \n",
    "                \n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print('Page not found: ' + species)\n",
    "        return None\n",
    "                    \n",
    "    except wikipedia.exceptions.DisambiguationError:\n",
    "        print(\"Ack! Disambiguation error.\")\n",
    "        return None\n",
    "        \n",
    "    #sometimes a KeyError is thrown for some strange reason...\n",
    "    except KeyError:\n",
    "        print(\"KeyError?\")\n",
    "        return None\n",
    "        \n",
    "    #return nothing if no diet types were found; otherwise, return the diet dictionary\n",
    "    if total_count == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return diet\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#######clean up the data a little bit##########\n",
    "#a function to consolidate the diet types found by the wiki_diets function (e.g. \"predator\", \"carnivorous\" collapsed into \"carnivore\")\n",
    "#also finds the most commonly mentioned diet type for each species\n",
    "\n",
    "def consolidate(diet_dict):\n",
    "    \n",
    "    #first, make a dictionary that consolidates the various vocab (carnivore, carnivorous, etc.) into one diet category\n",
    "    diet_key = {}\n",
    "    diet_key[(\"carnivore\", \"carnivorous\", 'Carnivorous', 'Carnivore', 'bird of prey', 'predatory', 'carnivora', 'amphibian', 'amphibians', 'carnivores', 'reptile', 'cephalopod')] = \"carnivore\"\n",
    "    diet_key[(\"herbivore\", \"herbivorous\", \"Herbivore\", \"Herbivorous\", 'iguana', 'tortoise')] = \"herbivore\"\n",
    "    diet_key[(\"omnivore\", \"omnivorous\", \"Omnivore\", \"Omnivorous\")] = \"omnivore\"\n",
    "    diet_key[(\"insectivore\", \"Insectivore\", \"insectivorous\", \"Insectivorous\", 'entomophagous', 'spider')] = \"insectivore\"\n",
    "    diet_key[(\"frugivore\", \"Frugivore\", \"frugivorous\", \"Frugivorous\")] = \"frugivore\"\n",
    "    diet_key[(\"blood feeder\", \"Blood feeder\", \"Hematophagy\", \"hematophagy\")] = \"blood feeder\"\n",
    "    diet_key[(\"Scavenge\", 'scavenge', 'detritivore', 'Detritivore', 'detritivorous', 'Detritivorous', 'carrion', 'Carrion', 'detrivorous')] = \"scavenger\"\n",
    "    diet_key[(\"granivore\", \"Granivore\", \"granivorous\", \"Granivorous\")] = \"granivore\"\n",
    "    diet_key[(\"nectarivore\", \"Nectarivore\", \"nectarivorous\", \"Nectarivorous\")] = \"nectarivore\"\n",
    "    diet_key[(\"folivore\", \"Folivore\", \"folivorous\", \"Folivorous\", \"folivory\", \"Folivory\")] = \"folivore\"\n",
    "    diet_key[(\"gummivore\", \"Gummivore\", \"gummivorous\", \"Gummivorous\", \"gummivory\", \"Gummivory\")] = \"gummivore\"\n",
    "    diet_key[(\"filter feeder\", \"Filter feeder\", \"filter feed\", \"Filter feed\", 'clam ', 'mollusk ')] = \"filter feeder\"\n",
    "    diet_key[(' eat', ' food ', ' consume', ' diet ', ' prey')] = \"undefined\"\n",
    "    \n",
    "\n",
    "    #then, establish a dictionary that will hold the consolidated diet types for each animal\n",
    "    categorized = {}\n",
    "    \n",
    "    #and go through the diet_dict to consolidate the diets of each species in diet_dict\n",
    "    for species in diet_dict:\n",
    "        categorized[species] = {}         #establish an entry for each species\n",
    "        \n",
    "        #for each diet of each species in diet_dict, look through diet_key and assign a category accordingly\n",
    "        if diet_dict[species] != None and diet_dict[species] != []:\n",
    "            for diet in diet_dict[species]: #['diet']:     #for each diet word listed under 'diet' \n",
    "          #      print(diet)\n",
    "                for item in diet_key:                  #go through the diet categories and find which one the diet word fits into\n",
    "                    if diet.lower() in item:\n",
    "                        categorized_entry = diet_key[item]\n",
    "                    \n",
    "                #if need be, establish an entry in 'categorized' for this diet type\n",
    "                if categorized_entry not in categorized[species]:\n",
    "                    categorized[species][categorized_entry] = {}\n",
    "                    categorized[species][categorized_entry]['count'] = 0\n",
    "                    categorized[species][categorized_entry]['sentences'] = []\n",
    "\n",
    "                #add the specific diet count to the category count, and append the relevant sentences\n",
    "                categorized[species][categorized_entry]['count'] += diet_dict[species][diet]['count']  #diet_dict[species]['diet'][diet]['count']\n",
    "                categorized[species][categorized_entry]['sentences'].append(diet_dict[species][diet]['mentions'])#(diet_dict[species]['diet'][diet]['mentions'])\n",
    "\n",
    "        #after going through all the diets per species, find which diet category has the most mentions\n",
    "        highest = 0\n",
    "        highest_cat = ''\n",
    "        for category in categorized[species]:\n",
    "            if categorized[species][category]['count'] > highest:\n",
    "                highest = categorized[species][category]['count']\n",
    "                highest_cat = category\n",
    "        categorized[species]['most_likely'] = highest_cat\n",
    "        \n",
    "\n",
    "    return categorized\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW CRUNCHING THROUGH LIST:  1 \n",
      "\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No tree found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No tree found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No overview found.\n",
      "No eats found.\n",
      "No diet section and no article diet data.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "No eats found.\n",
      "No species link found.\n",
      "No species link found.\n",
      "No eats found.\n",
      "Page not found: Trioceros melleri\n",
      "Page not found: Micronycteris brachyotis\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "fine\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n",
      "remains undefined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'species_test_file.csvall_diets_categorized.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_species_diet('species_test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtenv-py38",
   "language": "python",
   "name": "my-virtenv-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
